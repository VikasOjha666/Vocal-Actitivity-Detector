{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I repeated the below process couple of times to generate a large dataset.For generating data I took movies sound track and labelled the speech containing parts and stored the labelling in csv file having same name as that of audio track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "import os\n",
    "import warnings\n",
    "import h5py\n",
    "import librosa\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the portion from the audio track which contains speech which is well labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['track81','track82','track83']:\n",
    "    annotations=pd.read_csv('./Custom/{filename}.csv'.format(filename=name),header=None)\n",
    "\n",
    "    X_p=[]\n",
    "    X_n=[]\n",
    "    Y_p=[]\n",
    "    Y_n=[]\n",
    "    test=[]\n",
    "    count=0\n",
    "    #len(annotations)\n",
    "    for i in range(len(annotations)):\n",
    "        try:\n",
    "\n",
    "            print(\"Pass:%d\"%i)\n",
    "            _,start,end,lb=annotations.iloc[i]\n",
    "            start,end,lb=int(start),int(end),int(lb)\n",
    "\n",
    "            for j in range(abs(round((start-end))+1)):\n",
    "                if lb==1:\n",
    "                    audio,sr=librosa.load('./Custom/{filename}.mp3'.format(filename=name),offset=start,duration=1)\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                if lb==1:\n",
    "\n",
    "                    features=librosa.core.stft(audio,n_fft=1024,hop_length=256,win_length=1024)\n",
    "                    if features.shape[1]==87:\n",
    "                        X_p.append(features)\n",
    "                        Y_p.append(1)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "#                     features=librosa.core.stft(audio,n_fft=1024,hop_length=256,win_length=1024)\n",
    "#                     if features.shape[1]==87:\n",
    "#                         X_n.append(features)\n",
    "#                         Y_n.append(0)\n",
    "#                     else:\n",
    "#                         pass\n",
    "                start+=1\n",
    "                count+=1\n",
    "        except BaseException as e:\n",
    "            pass\n",
    "\n",
    "    X_p=np.array(X_p)\n",
    "    #X_n=np.array(X_n)\n",
    "    Y_p=np.array(Y_p)\n",
    "    #Y_n=np.array(Y_n)\n",
    "\n",
    "#     shuffle(X_p,Y_p)\n",
    "#     shuffle(X_n,Y_n)\n",
    "\n",
    "#     upto=min(len(Y_p),len(Y_n))\n",
    "\n",
    "#     X_n=X_n[0:upto]\n",
    "#     Y_n=Y_n[0:upto]\n",
    "\n",
    "#     X=np.concatenate((X_p,X_n))\n",
    "#     Y=np.concatenate((Y_p,Y_n))\n",
    "    print(X_p.shape)\n",
    "    np.save('./Custom/X_{filename}_p.npy'.format(filename=name),X_p)\n",
    "    np.save('./Custom/Y_{filename}_p.npy'.format(filename=name),Y_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting both positive and negative speech data.\n",
    "#### In actual practice I repeated this for a lot of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['track42','track43','track44','track45']:\n",
    "    annotations=pd.read_csv('{filename}.csv'.format(filename=name),header=None)\n",
    "\n",
    "    X_p=[]\n",
    "    X_n=[]\n",
    "    Y_p=[]\n",
    "    Y_n=[]\n",
    "    test=[]\n",
    "    count=0\n",
    "    #len(annotations)\n",
    "    for i in range(len(annotations)):\n",
    "        try:\n",
    "\n",
    "            print(\"Pass:%d\"%i)\n",
    "            _,start,end,lb=annotations.iloc[i]\n",
    "            start,end,lb=int(start),int(end),int(lb)\n",
    "\n",
    "            for j in range(abs(round((start-end))+1)):\n",
    "                audio,sr=librosa.load('{filename}.mp3'.format(filename=name),offset=start,duration=1)\n",
    "                if lb==1:\n",
    "\n",
    "                    features=librosa.core.stft(audio,n_fft=1024,hop_length=256,win_length=1024)\n",
    "                    if features.shape[1]==87:\n",
    "                        X_p.append(features)\n",
    "                        Y_p.append(1)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    \n",
    "\n",
    "                    features=librosa.core.stft(audio,n_fft=1024,hop_length=256,win_length=1024)\n",
    "                    if features.shape[1]==87:\n",
    "                        X_n.append(features)\n",
    "                        Y_n.append(0)\n",
    "                    else:\n",
    "                        pass\n",
    "                start+=1\n",
    "                count+=1\n",
    "        except BaseException as e:\n",
    "            pass\n",
    "\n",
    "    X_p=np.array(X_p)\n",
    "    X_n=np.array(X_n)\n",
    "    Y_p=np.array(Y_p)\n",
    "    Y_n=np.array(Y_n)\n",
    "\n",
    "    shuffle(X_p,Y_p)\n",
    "    shuffle(X_n,Y_n)\n",
    "\n",
    "    upto=min(len(Y_p),len(Y_n))\n",
    "\n",
    "    X_n=X_n[0:upto]\n",
    "    Y_n=Y_n[0:upto]\n",
    "\n",
    "    X=np.concatenate((X_p,X_n))\n",
    "    Y=np.concatenate((Y_p,Y_n))\n",
    "    print(X.shape)\n",
    "    np.save('X_{filename}.npy'.format(filename=name),X)\n",
    "    np.save('Y_{filename}.npy'.format(filename=name),Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging those numpy dumps to form dask array as we can't store that much arrays in low memory RAMs such as 8gb RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDaskArray_x(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load('./custom/'+arr_name)\n",
    "        X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "def createDaskArray_y(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load('./custom/'+arr_name)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "X=np.load('./custom/'+'X_track46_p.npy')\n",
    "X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "dax=da.from_array(X)\n",
    "del X\n",
    "\n",
    "Y=np.load('./custom/'+'Y_track46_p.npy')\n",
    "day=da.from_array(Y)\n",
    "del Y\n",
    "\n",
    "array_dumps_x=['X_track69_p.npy','X_track70_p.npy','X_track71_p.npy','X_track72_p.npy']\n",
    "\n",
    "array_dumps_y=['Y_track69_p.npy','Y_track70_p.npy','Y_track71_p.npy','Y_track72_p.npy']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "daf1_x=createDaskArray_x(dax,array_dumps_x)\n",
    "daf1_y=createDaskArray_y(day,array_dumps_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2023, 1, 513, 87, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daf1_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some negative samples from some other track in order to balance the negative samples in case of merging arrays only with positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(80,103):\n",
    "    audio,sr=librosa.load('E:\\\\HVD Dataset\\\\ADHM.mp3',sr=22050,duration=1)\n",
    "    features=librosa.core.stft(audio,n_fft=1024,hop_length=256,win_length=1024)\n",
    "    if features.shape[1]==87:\n",
    "        X_fil.append(features)\n",
    "        Y_fil.append(0)\n",
    "\n",
    "X_fil=np.array(X_fil)\n",
    "Y_fil=np.array(Y_fil)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fil=np.expand_dims(np.expand_dims(X_fil,axis=-1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 1, 513, 87, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDaskArray_x(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load('./custom/'+arr_name)\n",
    "        X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "def createDaskArray_y(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load('./custom/'+arr_name)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "X=np.load('./custom/'+'X_neg8.npy')\n",
    "X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "dax=da.from_array(X)\n",
    "del X\n",
    "\n",
    "Y=np.load('./custom/'+'Y_neg8.npy')\n",
    "day=da.from_array(Y)\n",
    "del Y\n",
    "\n",
    "array_dumps_x=['X_neg9.npy']\n",
    "array_dumps_y=['Y_neg9.npy']\n",
    "\n",
    "\n",
    "daf2_x=createDaskArray_x(dax,array_dumps_x)\n",
    "daf2_y=createDaskArray_y(day,array_dumps_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daf_c_x=da.concatenate([daf1_x,daf2_x],axis=0)\n",
    "daf_c_y=da.concatenate([daf1_y,daf2_y],axis=0)\n",
    "del daf1_x,daf2_x,daf1_y,daf2_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 1, 513, 87, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daf_c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "daf_c_x=da.concatenate([daf_c_x,X_fil],axis=0)\n",
    "daf_c_y=da.concatenate([daf_c_y,Y_fil],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4046, 1, 513, 87, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daf_c_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDaskArray_x(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load(arr_name)\n",
    "        X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "def createDaskArray_y(darr,npylist):\n",
    "    for arr_name in npylist:\n",
    "        X=np.load(arr_name)\n",
    "        darr=da.concatenate([X,darr],axis=0)\n",
    "        del X\n",
    "    return darr\n",
    "\n",
    "\n",
    "\n",
    "X=np.load('X_track14.npy')\n",
    "X=np.expand_dims(np.expand_dims(X,axis=-1),axis=1)\n",
    "dax=da.from_array(X,chunks='auto')\n",
    "del X\n",
    "\n",
    "Y=np.load('Y_track14.npy')\n",
    "day=da.from_array(Y,chunks='auto')\n",
    "del Y\n",
    "\n",
    "array_dumps_x=['X_track15.npy','X_track16.npy','X_track17.npy','X_track42.npy','X_track43.npy','X_track44.npy',\n",
    "              'X_track45.npy']\n",
    "array_dumps_y=['Y_track15.npy','Y_track16.npy','Y_track17.npy','Y_track42.npy','Y_track43.npy','Y_track44.npy',\n",
    "              'Y_track45.npy']\n",
    "\n",
    "dafv_x=createDaskArray_x(dax,array_dumps_x)\n",
    "dafv_y=createDaskArray_y(day,array_dumps_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "davf_x=da.concatenate([daf_c_x,dafv_x],axis=0)\n",
    "davf_y=da.concatenate([daf_c_y,dafv_y],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8745,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davf_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping the dask arrays as hdf5 arrays as hdf5 can store arrays as file in disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hdf5_store=h5py.File('./X_val1.hdf5','a')\n",
    "results=hdf5_store.create_dataset(\"results\",data=davf_x)\n",
    "hdf5_store.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del daf_c_x\n",
    "hdf5_store=h5py.File('./Y_val1.hdf5','a')\n",
    "results=hdf5_store.create_dataset(\"results\",data=davf_y)\n",
    "hdf5_store.close()\n",
    "del daf_c_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates negative samples from some tracks which contains only background sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the negative samples.\n",
    "files_list=os.listdir('./custom/Background/')\n",
    "\n",
    "def remove_files(lis):\n",
    "    for filename in lis:\n",
    "        os.remove('./custom/Background/'+filename)\n",
    "X=[]\n",
    "Y=[]\n",
    "file_no=1\n",
    "files_to_remove=[]\n",
    "for filename in files_list:\n",
    "    audio,sr=librosa.load('./custom/Background/'+filename)\n",
    "    duration=int(librosa.core.get_duration(audio,sr))\n",
    "    del audio,sr\n",
    "    \n",
    "    for i in range(duration):\n",
    "        a,s=librosa.load('./custom/Background/'+filename,offset=i,duration=1)\n",
    "        features=librosa.core.stft(a,n_fft=1024,hop_length=256,win_length=1024)\n",
    "        if features.shape[1]==87:\n",
    "            X.append(features)\n",
    "            Y.append(0)\n",
    "        \n",
    "        if len(X)==6000:\n",
    "            X=np.array(X)\n",
    "            Y=np.array(Y)\n",
    "            np.save(f'custom/X_neg{file_no}.npy',X)\n",
    "            np.save(f'custom/Y_neg{file_no}.npy',Y)\n",
    "            print(f'Created X_neg{file_no}.npy and Y_neg{file_no}.npy')\n",
    "            #print(files_to_remove)\n",
    "            remove_files(files_to_remove)\n",
    "            files_to_remove=[]\n",
    "            X=[]\n",
    "            Y=[]\n",
    "            \n",
    "            file_no+=1\n",
    "    files_to_remove.append(filename)\n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
